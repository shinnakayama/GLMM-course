{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPr+wb2siQFm7m7ziigw0QH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shinnakayama/GLMM-course/blob/master/transshipment_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "!echo \"deb http://packages.cloud.google.com/apt gcsfuse-bionic main\" > /etc/apt/sources.list.d/gcsfuse.list\n",
        "!curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -\n",
        "!apt -qq update\n",
        "!apt -qq install gcsfuse\n",
        "\n",
        "# create a folder to mount\n",
        "temp_folder_name = 'temp/'\n",
        "if not os.path.exists(temp_folder_name):\n",
        "    os.mkdir(temp_folder_name)\n",
        "\n",
        "# mount\n",
        "my_bucket = 'planet_imagery'\n",
        "!gcsfuse --implicit-dirs {my_bucket} {temp_folder_name}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eKMUlw-rEsPR",
        "outputId": "b5351e7c-b9e3-4553-b24c-8b9e0222a111"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0Warning: apt-key is deprecated. Manage keyring files in trusted.gpg.d instead (see apt-key(8)).\n",
            "\r100  1022  100  1022    0     0  15336      0 --:--:-- --:--:-- --:--:-- 15484\n",
            "OK\n",
            "86 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mhttp://packages.cloud.google.com/apt/dists/gcsfuse-bionic/InRelease: Key is stored in legacy trusted.gpg keyring (/etc/apt/trusted.gpg), see the DEPRECATION section in apt-key(8) for details.\u001b[0m\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "gcsfuse is already the newest version (2.4.0).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 86 not upgraded.\n",
            "{\"timestamp\":{\"seconds\":1722554243,\"nanos\":249333428},\"severity\":\"INFO\",\"message\":\"Start gcsfuse/2.4.0 (Go version go1.22.4) for app \\\"\\\" using mount point: /content/temp\\n\"}\n",
            "{\"timestamp\":{\"seconds\":1722554243,\"nanos\":249523027},\"severity\":\"INFO\",\"message\":\"GCSFuse mount command flags: {\\\"AppName\\\":\\\"\\\",\\\"Foreground\\\":false,\\\"ConfigFile\\\":\\\"\\\",\\\"MountOptions\\\":{},\\\"DirMode\\\":493,\\\"FileMode\\\":420,\\\"Uid\\\":-1,\\\"Gid\\\":-1,\\\"ImplicitDirs\\\":true,\\\"OnlyDir\\\":\\\"\\\",\\\"RenameDirLimit\\\":0,\\\"IgnoreInterrupts\\\":true,\\\"CustomEndpoint\\\":null,\\\"BillingProject\\\":\\\"\\\",\\\"KeyFile\\\":\\\"\\\",\\\"TokenUrl\\\":\\\"\\\",\\\"ReuseTokenFromUrl\\\":true,\\\"EgressBandwidthLimitBytesPerSecond\\\":-1,\\\"OpRateLimitHz\\\":-1,\\\"SequentialReadSizeMb\\\":200,\\\"AnonymousAccess\\\":false,\\\"MaxRetrySleep\\\":30000000000,\\\"MaxRetryAttempts\\\":0,\\\"StatCacheCapacity\\\":20460,\\\"StatCacheTTL\\\":60000000000,\\\"TypeCacheTTL\\\":60000000000,\\\"KernelListCacheTtlSeconds\\\":0,\\\"HttpClientTimeout\\\":0,\\\"RetryMultiplier\\\":2,\\\"TempDir\\\":\\\"\\\",\\\"ClientProtocol\\\":\\\"http1\\\",\\\"MaxConnsPerHost\\\":0,\\\"MaxIdleConnsPerHost\\\":100,\\\"EnableNonexistentTypeCache\\\":false,\\\"StackdriverExportInterval\\\":0,\\\"PrometheusPort\\\":0,\\\"OtelCollectorAddress\\\":\\\"\\\",\\\"LogFile\\\":\\\"\\\",\\\"LogFormat\\\":\\\"json\\\",\\\"ExperimentalEnableJsonRead\\\":false,\\\"DebugFuse\\\":false,\\\"DebugGCS\\\":false,\\\"DebugInvariants\\\":false,\\\"DebugMutex\\\":false,\\\"ExperimentalMetadataPrefetchOnMount\\\":\\\"disabled\\\"}\"}\n",
            "{\"timestamp\":{\"seconds\":1722554243,\"nanos\":249613033},\"severity\":\"INFO\",\"message\":\"GCSFuse mount config flags: {\\\"CreateEmptyFile\\\":false,\\\"Severity\\\":\\\"INFO\\\",\\\"Format\\\":\\\"\\\",\\\"FilePath\\\":\\\"\\\",\\\"LogRotateConfig\\\":{\\\"MaxFileSizeMB\\\":512,\\\"BackupFileCount\\\":10,\\\"Compress\\\":true},\\\"MaxSizeMB\\\":-1,\\\"CacheFileForRangeRead\\\":false,\\\"EnableParallelDownloads\\\":false,\\\"ParallelDownloadsPerFile\\\":16,\\\"MaxParallelDownloads\\\":16,\\\"DownloadChunkSizeMB\\\":50,\\\"EnableCRC\\\":false,\\\"CacheDir\\\":\\\"\\\",\\\"TtlInSeconds\\\":-9223372036854775808,\\\"TypeCacheMaxSizeMB\\\":4,\\\"StatCacheMaxSizeMB\\\":-9223372036854775808,\\\"EnableEmptyManagedFolders\\\":false,\\\"GRPCConnPoolSize\\\":1,\\\"AnonymousAccess\\\":false,\\\"EnableHNS\\\":false,\\\"IgnoreInterrupts\\\":true,\\\"DisableParallelDirops\\\":false,\\\"KernelListCacheTtlSeconds\\\":0,\\\"MaxRetryAttempts\\\":0,\\\"PrometheusPort\\\":0}\"}\n",
            "{\"timestamp\":{\"seconds\":1722554243,\"nanos\":320717847},\"severity\":\"INFO\",\"message\":\"File system has been successfully mounted.\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_V2_Weights\n",
        "from torchvision.models.detection.rpn import AnchorGenerator, RPNHead\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from pycocotools.coco import COCO\n",
        "from pycocotools.cocoeval import COCOeval\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import contextlib\n",
        "import io\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches"
      ],
      "metadata": {
        "id": "WNKS0651PBim"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, json_file, root_dir, transforms=None):\n",
        "        with open(json_file) as f:\n",
        "            self.data = json.load(f)\n",
        "        self.root_dir = root_dir\n",
        "        self.transforms = transforms\n",
        "\n",
        "        self.imgs = self.data['images']\n",
        "        self.annotations = self.data['annotations']\n",
        "        self.categories = self.data['categories']\n",
        "\n",
        "        self.img_to_anns = {img['id']: [] for img in self.imgs}\n",
        "        for ann in self.annotations:\n",
        "            self.img_to_anns[ann['image_id']].append(ann)\n",
        "\n",
        "        self.imgs = [img for img in self.imgs if self.img_to_anns[img['id']]]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_info = self.imgs[idx]\n",
        "        img_path = os.path.join(self.root_dir, img_info['file_name'])\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        annots = self.img_to_anns[img_info['id']]\n",
        "\n",
        "        boxes = []\n",
        "        labels = []\n",
        "        for annot in annots:\n",
        "            xmin, ymin, width, height = annot['bbox']\n",
        "            boxes.append([xmin, ymin, xmin + width, ymin + height])\n",
        "            labels.append(annot['category_id'])\n",
        "\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
        "        image_id = torch.tensor([img_info['id']])\n",
        "        area = torch.as_tensor([annot['area'] for annot in annots], dtype=torch.float32)\n",
        "        iscrowd = torch.as_tensor([annot['iscrowd'] for annot in annots], dtype=torch.int64)\n",
        "\n",
        "        target = {}\n",
        "        target['boxes'] = boxes\n",
        "        target['labels'] = labels\n",
        "        target['image_id'] = image_id\n",
        "\n",
        "        if self.transforms is not None:\n",
        "            img = self.transforms(img)\n",
        "\n",
        "        return img, target"
      ],
      "metadata": {
        "id": "f6rDG09qPNx6"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_mean_std(root_dir):\n",
        "    means = []\n",
        "    stds = []\n",
        "    transform = transforms.ToTensor()\n",
        "    for img_file in tqdm(os.listdir(root_dir)):\n",
        "        img_path = os.path.join(root_dir, img_file)\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        tensor = transform(image)\n",
        "        means.append(tensor.mean(dim=[1, 2]))\n",
        "        stds.append(tensor.std(dim=[1, 2]))\n",
        "\n",
        "    mean = torch.stack(means).mean(dim=0)\n",
        "    std = torch.stack(stds).mean(dim=0)\n",
        "\n",
        "    return mean, std\n",
        "\n",
        "root_dir = 'temp/majuro_2022/chopped'\n",
        "mean, std = calculate_mean_std(root_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_NUQqT1kPUEe",
        "outputId": "ffdc7bda-5e98-45e8-c96e-263a7657678d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 41/41 [00:10<00:00,  3.82it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_transform(train, mean, std):\n",
        "    transforms_list = []\n",
        "    transforms_list.append(transforms.ToTensor())\n",
        "    if train:\n",
        "        transforms_list.append(transforms.RandomHorizontalFlip(0.5))\n",
        "        transforms_list.append(transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2))\n",
        "    transforms_list.append(transforms.Normalize(mean=mean.tolist(), std=std.tolist()))\n",
        "    return transforms.Compose(transforms_list)"
      ],
      "metadata": {
        "id": "EU5u7XEoRfmk"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "    return tuple(zip(*batch))"
      ],
      "metadata": {
        "id": "WgqkDFFbRigV"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_image(img, target, prediction):\n",
        "    img = img.permute(1, 2, 0).cpu().numpy()\n",
        "    fig, ax = plt.subplots(1, figsize=(12, 9))\n",
        "    ax.imshow(img)\n",
        "\n",
        "    # Plot ground truth boxes\n",
        "    for box in target['boxes']:\n",
        "        xmin, ymin, xmax, ymax = box.cpu().numpy()\n",
        "        width, height = xmax - xmin\n",
        "        height = ymax - ymin\n",
        "        rect = patches.Rectangle((xmin, ymin), width, height, linewidth=2, edgecolor='g', facecolor='none')\n",
        "        ax.add_patch(rect)\n",
        "\n",
        "    # Plot predicted boxes\n",
        "    for box in prediction['boxes']:\n",
        "        xmin, ymin, xmax, ymax = box.cpu().numpy()\n",
        "        width, height = xmax - xmin\n",
        "        height = ymax - ymin\n",
        "        rect = patches.Rectangle((xmin, ymin), width, height, linewidth=2, edgecolor='r', facecolor='none')\n",
        "        ax.add_patch(rect)\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "iSFkkByARl-p"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, data_loader, device, coco_gt):\n",
        "    model.eval()\n",
        "    coco_results = []\n",
        "    with torch.no_grad():\n",
        "        for images, targets in data_loader:\n",
        "            images = list(img.to(device) for img in images)\n",
        "            outputs = model(images)\n",
        "            for target, output in zip(targets, outputs):\n",
        "                image_id = target[\"image_id\"].item()\n",
        "                boxes = output[\"boxes\"].cpu().numpy()\n",
        "                scores = output[\"scores\"].cpu().numpy()\n",
        "                labels = output[\"labels\"].cpu().numpy()\n",
        "\n",
        "                for box, score, label in zip(boxes, scores, labels):\n",
        "                    xmin, ymin, xmax, ymax = box\n",
        "                    width = xmax - xmin\n",
        "                    height = ymax - ymin\n",
        "                    coco_results.append({\n",
        "                        \"image_id\": int(image_id),\n",
        "                        \"category_id\": int(label),\n",
        "                        \"bbox\": [float(xmin), float(ymin), float(width), float(height)],\n",
        "                        \"score\": float(score)\n",
        "                    })\n",
        "\n",
        "    with open(\"results.json\", \"w\") as f:\n",
        "        json.dump(coco_results, f, indent=4)\n",
        "\n",
        "    # Suppress COCOeval output\n",
        "    with contextlib.redirect_stdout(io.StringIO()):\n",
        "        coco_dt = coco_gt.loadRes(\"results.json\")\n",
        "        coco_eval = COCOeval(coco_gt, coco_dt, \"bbox\")\n",
        "        coco_eval.params.iouThrs = np.array([0.5])  # Set IoU threshold to 0.5\n",
        "        coco_eval.evaluate()\n",
        "        coco_eval.accumulate()\n",
        "        coco_eval.summarize()\n",
        "\n",
        "    # Prepare AP per class\n",
        "    precisions = coco_eval.eval['precision']\n",
        "    classes = [cat['name'] for cat in coco_gt.loadCats(coco_gt.getCatIds())]\n",
        "    ap_per_class = {}\n",
        "    for idx, class_name in enumerate(classes):\n",
        "        # Calculate AP at IoU threshold 0.5\n",
        "        ap = np.mean(precisions[:, :, idx, 0, -1])\n",
        "        ap_per_class[class_name] = ap\n",
        "\n",
        "    ap_table = pd.DataFrame(list(ap_per_class.items()), columns=['Class', 'AP'])\n",
        "    mAP = coco_eval.stats[0]\n",
        "\n",
        "    return mAP, ap_table, coco_results"
      ],
      "metadata": {
        "id": "zO2avgujRpQ9"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_classes(dataset):\n",
        "    class_counts = Counter()\n",
        "    for img, target in dataset:\n",
        "        labels = target['labels'].numpy()\n",
        "        class_counts.update(labels)\n",
        "    return class_counts"
      ],
      "metadata": {
        "id": "UMXIp0MWRsRw"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_results(model_path, dataset, device):\n",
        "    # Load the trained model\n",
        "    model = fasterrcnn_resnet50_fpn_v2(weights=None)\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    num_classes = len(dataset.categories) + 1\n",
        "    model.roi_heads.box_predictor = torch.nn.Linear(in_features, num_classes)\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # DataLoader for the dataset\n",
        "    data_loader = DataLoader(dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "    # Visualize the test images with predictions\n",
        "    for images, targets in data_loader:\n",
        "        images = list(img.to(device) for img in images)\n",
        "        outputs = model(images)\n",
        "        for img, target, output in zip(images, targets, outputs):\n",
        "            visualize_image(img, target, output)"
      ],
      "metadata": {
        "id": "YKhnB0Q_Rugw"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "    # Load the custom dataset\n",
        "    dataset = CustomDataset(json_file='instances_default.json', root_dir='temp/majuro_2022/chopped', transforms=get_transform(train=True, mean=mean, std=std))\n",
        "\n",
        "    # Determine split sizes\n",
        "    train_size = int(0.8 * len(dataset))\n",
        "    test_size = len(dataset) - train_size\n",
        "\n",
        "    # Split the dataset into train and test set\n",
        "    dataset_train, dataset_test = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "    # Count the number of each class in the training dataset\n",
        "    class_counts_train = count_classes(dataset_train)\n",
        "    print(\"Class counts in the training dataset:\")\n",
        "    for class_id, count in class_counts_train.items():\n",
        "        class_name = next(cat['name'] for cat in dataset.categories if cat['id'] == class_id)\n",
        "        print(f\"{class_name}: {count}\")\n",
        "\n",
        "    # Count the number of each class in the test dataset\n",
        "    class_counts_test = count_classes(dataset_test)\n",
        "    print(\"\\nClass counts in the test dataset:\")\n",
        "    for class_id, count in class_counts_test.items():\n",
        "        class_name = next(cat['name'] for cat in dataset.categories if cat['id'] == class_id)\n",
        "        print(f\"{class_name}: {count}\")\n",
        "\n",
        "    # Update transforms for the test set\n",
        "    dataset_test.dataset.transforms = get_transform(train=False, mean=mean, std=std)\n",
        "\n",
        "    data_loader = DataLoader(dataset_train, batch_size=2, shuffle=True, num_workers=4, collate_fn=collate_fn)\n",
        "    data_loader_test = DataLoader(dataset_test, batch_size=1, shuffle=False, num_workers=4, collate_fn=collate_fn)\n",
        "\n",
        "    # Load a pre-trained model with specific weights\n",
        "    weights = FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT\n",
        "    model = fasterrcnn_resnet50_fpn_v2(weights=weights)\n",
        "\n",
        "    # Define custom anchor generator with sizes and aspect ratios\n",
        "    anchor_generator = AnchorGenerator(\n",
        "        sizes=((4,), (8,), (16,), (32,), (64,)),\n",
        "        aspect_ratios=((0.5, 2.0, 4.0),) * 5\n",
        "    )\n",
        "\n",
        "    # Replace the RPN anchor generator with the custom one\n",
        "    model.rpn.anchor_generator = anchor_generator\n",
        "\n",
        "    # Optionally replace the RPN head if needed\n",
        "    model.rpn.head = RPNHead(256, anchor_generator.num_anchors_per_location()[0])\n",
        "\n",
        "    # Get the number of input features for the classifier\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "\n",
        "    # Replace the pre-trained head with a new one (num_classes includes the background)\n",
        "    num_classes = len(dataset.categories) + 1\n",
        "    model.roi_heads.box_predictor = torch.nn.Linear(in_features, num_classes)\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    # Construct an optimizer\n",
        "    params = [p for p in model.parameters() if p.requires_grad]\n",
        "    optimizer = torch.optim.SGD(params, lr=0.001, momentum=0.9, weight_decay=0.0005)\n",
        "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
        "\n",
        "    num_epochs = 10\n",
        "\n",
        "    # Load COCO ground truth for evaluation\n",
        "    coco_gt = COCO('instances_default.json')\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for images, targets in tqdm(data_loader, desc=f\"Training Epoch {epoch+1}/{num_epochs}\"):\n",
        "            images = list(image.to(device) for image in images)\n",
        "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "            loss_dict = model(images, targets)\n",
        "            losses = sum(loss for loss in loss_dict.values())\n",
        "            running_loss += losses.item()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            losses.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        lr_scheduler.step()\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(data_loader):.4f}\")\n",
        "\n",
        "        # Evaluate every 3 epochs\n",
        "        if (epoch + 1) % 3 == 0:\n",
        "            mAP, ap_table, _ = evaluate(model, data_loader_test, device, coco_gt)\n",
        "            print(f\"Epoch [{epoch+1}/{num_epochs}], mAP: {mAP:.4f}\")\n",
        "            print(ap_table)\n",
        "\n",
        "    # Final evaluation after all epochs\n",
        "    mAP, ap_table, predictions = evaluate(model, data_loader_test, device, coco_gt)\n",
        "    print(f\"Final Evaluation, mAP: {mAP:.4f}\")\n",
        "    print(ap_table)\n",
        "\n",
        "    # Save the model\n",
        "    torch.save(model.state_dict(), \"fasterrcnn_model.pth\")\n",
        "\n",
        "    # Now, you can call visualize_results when you want to visualize the results\n",
        "    # Example: visualize_results(\"fasterrcnn_model.pth\", dataset_test, device)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 625
        },
        "id": "bWub1e2wRwve",
        "outputId": "83a01015-4e34-4ecf-9317-0ffedcf939d1"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class counts in the training dataset:\n",
            "vessel1: 92\n",
            "vessel2: 24\n",
            "vessel3: 14\n",
            "\n",
            "Class counts in the test dataset:\n",
            "vessel2: 8\n",
            "vessel1: 28\n",
            "vessel3: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_v2_coco-dd69338a.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_v2_coco-dd69338a.pth\n",
            "100%|██████████| 167M/167M [00:02<00:00, 66.0MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.00s)\n",
            "creating index...\n",
            "index created!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1/10:   0%|          | 0/15 [00:03<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "too many values to unpack (expected 2)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-1a8956f5b6ad>\u001b[0m in \u001b[0;36m<cell line: 105>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-15-1a8956f5b6ad>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mloss_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m             \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloss_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/models/detection/generalized_rcnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"0\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0mproposals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproposal_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrpn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0mdetections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetector_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroi_heads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproposals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0mdetections\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_image_sizes\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[operator]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/models/detection/roi_heads.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, features, proposals, image_shapes, targets)\u001b[0m\n\u001b[1;32m    761\u001b[0m         \u001b[0mbox_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbox_roi_pool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproposals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m         \u001b[0mbox_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbox_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbox_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 763\u001b[0;31m         \u001b[0mclass_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox_regression\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbox_predictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbox_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    764\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m         \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
          ]
        }
      ]
    }
  ]
}